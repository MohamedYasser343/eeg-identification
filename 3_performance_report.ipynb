{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Person Identification - Part 3: Performance Analysis & Visualization\n",
    "\n",
    "## Overview\n",
    "This notebook provides comprehensive analysis of the trained CNN+RNN model:\n",
    "1. Load trained model and test results\n",
    "2. Detailed confusion matrix analysis\n",
    "3. Per-subject performance breakdown\n",
    "4. Feature visualization with t-SNE\n",
    "5. Error analysis and discussion\n",
    "6. Model interpretability insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import h5py\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Metrics and Visualization\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, f1_score, precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_FILE = 'data/processed/preprocessed_data.h5'\n",
    "MODEL_DIR = 'models'\n",
    "FIGURES_DIR = 'figures'\n",
    "\n",
    "# Create figures directory\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "# Find the most recent model\n",
    "model_files = [f for f in os.listdir(MODEL_DIR) if f.endswith('.keras')]\n",
    "if not model_files:\n",
    "    raise FileNotFoundError(\"No trained model found. Please run 2_model_training.ipynb first.\")\n",
    "\n",
    "latest_model = sorted(model_files)[-1]\n",
    "model_path = os.path.join(MODEL_DIR, latest_model)\n",
    "results_file = model_path.replace('.keras', '_results.json')\n",
    "\n",
    "print(f\"Loading model: {latest_model}\")\n",
    "\n",
    "# Load model\n",
    "model = keras.models.load_model(model_path)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Load results if available\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    print(f\"\\nPrevious Results:\")\n",
    "    print(f\"  Test Accuracy: {results['test_metrics']['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score (Macro): {results['test_metrics']['f1_macro']:.4f}\")\n",
    "\n",
    "# Load test data\n",
    "print(\"\\nLoading test data...\")\n",
    "with h5py.File(DATA_FILE, 'r') as f:\n",
    "    X_test = f['X_test'][:]\n",
    "    y_test = f['y_test'][:]\n",
    "    n_subjects = f.attrs['n_subjects']\n",
    "\n",
    "# Add channel dimension if needed\n",
    "if X_test.ndim == 4:\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Number of subjects: {n_subjects}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "print(\"Generating predictions on test set...\")\n",
    "y_pred_probs = model.predict(X_test, batch_size=32, verbose=1)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Get top-5 predictions for each sample\n",
    "top5_pred = np.argsort(y_pred_probs, axis=1)[:, -5:][:, ::-1]\n",
    "\n",
    "# Calculate top-5 accuracy\n",
    "top5_correct = np.array([y_test[i] in top5_pred[i] for i in range(len(y_test))])\n",
    "top5_accuracy = top5_correct.mean()\n",
    "\n",
    "print(f\"\\nPredictions complete!\")\n",
    "print(f\"  Test samples: {len(y_test)}\")\n",
    "print(f\"  Top-5 Accuracy: {top5_accuracy:.4f} ({top5_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overall Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "# Calculate per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*70)\n",
    "print(\"OVERALL PERFORMANCE METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nClassification Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Top-5 Accuracy: {top5_accuracy:.4f} ({top5_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nF1-Score Metrics:\")\n",
    "print(f\"  Macro Average: {f1_macro:.4f}\")\n",
    "print(f\"  Micro Average: {f1_micro:.4f}\")\n",
    "print(f\"  Weighted Average: {f1_weighted:.4f}\")\n",
    "print(f\"\\nPer-Class Statistics:\")\n",
    "print(f\"  Mean Precision: {precision.mean():.4f} ± {precision.std():.4f}\")\n",
    "print(f\"  Mean Recall: {recall.mean():.4f} ± {recall.std():.4f}\")\n",
    "print(f\"  Mean F1-Score: {f1.mean():.4f} ± {f1.std():.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create metrics visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "metrics_names = ['Top-1\\nAccuracy', 'Top-5\\nAccuracy']\n",
    "metrics_values = [accuracy, top5_accuracy]\n",
    "colors = ['#3498db', '#2ecc71']\n",
    "\n",
    "axes[0].bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Accuracy Metrics', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(metrics_values):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# F1-Score comparison\n",
    "f1_names = ['Macro', 'Micro', 'Weighted']\n",
    "f1_values = [f1_macro, f1_micro, f1_weighted]\n",
    "colors = ['#e74c3c', '#f39c12', '#9b59b6']\n",
    "\n",
    "axes[1].bar(f1_names, f1_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1].set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('F1-Score Variants', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(f1_values):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Distribution of per-class F1-scores\n",
    "axes[2].hist(f1, bins=30, alpha=0.7, edgecolor='black', color='#16a085')\n",
    "axes[2].axvline(f1.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {f1.mean():.3f}')\n",
    "axes[2].set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('Distribution of Per-Subject F1-Scores', fontsize=14, fontweight='bold')\n",
    "axes[2].legend(fontsize=10)\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'overall_metrics.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot confusion matrix with better visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(24, 10))\n",
    "\n",
    "# Absolute counts\n",
    "im1 = axes[0].imshow(cm, cmap='Blues', aspect='auto', interpolation='nearest')\n",
    "axes[0].set_xlabel('Predicted Subject ID', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('True Subject ID', fontsize=14, fontweight='bold')\n",
    "axes[0].set_title('Confusion Matrix (Absolute Counts)', fontsize=16, fontweight='bold')\n",
    "cbar1 = plt.colorbar(im1, ax=axes[0])\n",
    "cbar1.set_label('Count', fontsize=12)\n",
    "\n",
    "# Normalized (percentages)\n",
    "im2 = axes[1].imshow(cm_normalized, cmap='RdYlGn', aspect='auto', \n",
    "                     interpolation='nearest', vmin=0, vmax=1)\n",
    "axes[1].set_xlabel('Predicted Subject ID', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('True Subject ID', fontsize=14, fontweight='bold')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=16, fontweight='bold')\n",
    "cbar2 = plt.colorbar(im2, ax=axes[1])\n",
    "cbar2.set_label('Proportion', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'confusion_matrix_detailed.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Analyze most confused pairs\n",
    "print(\"\\nMost Confused Subject Pairs:\")\n",
    "print(\"=\"*60)\n",
    "confusion_pairs = []\n",
    "for i in range(n_subjects):\n",
    "    for j in range(n_subjects):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confusion_pairs.append((i, j, cm[i, j]))\n",
    "\n",
    "confusion_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "for i, (true_sub, pred_sub, count) in enumerate(confusion_pairs[:10]):\n",
    "    print(f\"{i+1}. Subject {true_sub} → Subject {pred_sub}: {count} times\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Per-Subject Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed per-subject analysis\n",
    "subject_metrics = []\n",
    "for subject_id in range(n_subjects):\n",
    "    # Get samples for this subject\n",
    "    subject_mask = (y_test == subject_id)\n",
    "    if subject_mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    subject_metrics.append({\n",
    "        'Subject_ID': subject_id,\n",
    "        'Test_Samples': support[subject_id],\n",
    "        'Precision': precision[subject_id],\n",
    "        'Recall': recall[subject_id],\n",
    "        'F1_Score': f1[subject_id],\n",
    "        'Correct_Predictions': cm[subject_id, subject_id],\n",
    "        'Accuracy': cm[subject_id, subject_id] / support[subject_id] if support[subject_id] > 0 else 0\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics = pd.DataFrame(subject_metrics)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nPer-Subject Performance Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(df_metrics.describe())\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Identify best and worst performing subjects\n",
    "best_subjects = df_metrics.nlargest(10, 'F1_Score')\n",
    "worst_subjects = df_metrics.nsmallest(10, 'F1_Score')\n",
    "\n",
    "print(\"\\nTop 10 Best Performing Subjects:\")\n",
    "print(best_subjects.to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 10 Worst Performing Subjects:\")\n",
    "print(worst_subjects.to_string(index=False))\n",
    "\n",
    "# Visualize per-subject metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# F1-Score by subject\n",
    "axes[0, 0].bar(df_metrics['Subject_ID'], df_metrics['F1_Score'], \n",
    "               alpha=0.7, edgecolor='black', color='steelblue')\n",
    "axes[0, 0].axhline(df_metrics['F1_Score'].mean(), color='red', \n",
    "                   linestyle='--', linewidth=2, label=f'Mean: {df_metrics[\"F1_Score\"].mean():.3f}')\n",
    "axes[0, 0].set_xlabel('Subject ID', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('F1-Score per Subject', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Precision vs Recall\n",
    "axes[0, 1].scatter(df_metrics['Recall'], df_metrics['Precision'], \n",
    "                   alpha=0.6, s=100, c=df_metrics['F1_Score'], \n",
    "                   cmap='viridis', edgecolors='black', linewidth=1)\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'r--', alpha=0.3, linewidth=2)\n",
    "axes[0, 1].set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Precision vs Recall (colored by F1)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "cbar = plt.colorbar(axes[0, 1].collections[0], ax=axes[0, 1])\n",
    "cbar.set_label('F1-Score', fontsize=10)\n",
    "\n",
    "# Accuracy vs Test Samples\n",
    "axes[1, 0].scatter(df_metrics['Test_Samples'], df_metrics['Accuracy'], \n",
    "                   alpha=0.6, s=100, c='coral', edgecolors='black', linewidth=1)\n",
    "axes[1, 0].set_xlabel('Number of Test Samples', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Accuracy vs Sample Size', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Distribution of metrics\n",
    "metrics_to_plot = ['Precision', 'Recall', 'F1_Score']\n",
    "df_metrics[metrics_to_plot].boxplot(ax=axes[1, 1], patch_artist=True)\n",
    "axes[1, 1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('Distribution of Performance Metrics', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'per_subject_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save metrics to CSV\n",
    "df_metrics.to_csv(os.path.join(FIGURES_DIR, 'per_subject_metrics.csv'), index=False)\n",
    "print(f\"\\nPer-subject metrics saved to: {os.path.join(FIGURES_DIR, 'per_subject_metrics.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Visualization with t-SNE\n",
    "\n",
    "Visualize the learned representations using t-SNE dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the model (before final classification layer)\n",
    "print(\"Extracting features from model...\")\n",
    "feature_extractor = keras.Model(\n",
    "    inputs=model.input,\n",
    "    outputs=model.layers[-3].output  # Layer before final dense layer\n",
    ")\n",
    "\n",
    "# Extract features for a subset of test data (for visualization efficiency)\n",
    "n_samples_per_subject = 5\n",
    "sample_indices = []\n",
    "for subject_id in range(n_subjects):\n",
    "    subject_mask = (y_test == subject_id)\n",
    "    subject_indices = np.where(subject_mask)[0]\n",
    "    if len(subject_indices) >= n_samples_per_subject:\n",
    "        sample_indices.extend(np.random.choice(subject_indices, n_samples_per_subject, replace=False))\n",
    "    else:\n",
    "        sample_indices.extend(subject_indices)\n",
    "\n",
    "X_sample = X_test[sample_indices]\n",
    "y_sample = y_test[sample_indices]\n",
    "\n",
    "print(f\"Extracting features for {len(X_sample)} samples...\")\n",
    "features = feature_extractor.predict(X_sample, batch_size=32, verbose=1)\n",
    "print(f\"Feature shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA first to reduce dimensionality (faster t-SNE)\n",
    "print(\"\\nApplying PCA for initial dimensionality reduction...\")\n",
    "pca = PCA(n_components=50)\n",
    "features_pca = pca.fit_transform(features)\n",
    "print(f\"PCA variance explained: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Apply t-SNE\n",
    "print(\"\\nApplying t-SNE (this may take several minutes)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000, verbose=1)\n",
    "features_tsne = tsne.fit_transform(features_pca)\n",
    "print(\"t-SNE complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize t-SNE embedding\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot 1: Colored by subject (with legend for subset)\n",
    "unique_subjects = np.unique(y_sample)\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(unique_subjects)))\n",
    "\n",
    "for idx, subject_id in enumerate(unique_subjects[:20]):  # Show first 20 in legend\n",
    "    mask = (y_sample == subject_id)\n",
    "    axes[0].scatter(features_tsne[mask, 0], features_tsne[mask, 1], \n",
    "                   c=[colors[idx]], label=f'Subject {subject_id}', \n",
    "                   alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Plot remaining subjects without legend\n",
    "for subject_id in unique_subjects[20:]:\n",
    "    mask = (y_sample == subject_id)\n",
    "    axes[0].scatter(features_tsne[mask, 0], features_tsne[mask, 1], \n",
    "                   alpha=0.4, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "axes[0].set_xlabel('t-SNE Dimension 1', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('t-SNE Dimension 2', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('t-SNE Visualization of Learned Features\\n(Colored by Subject)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8, ncol=2)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Heatmap visualization\n",
    "from scipy.stats import gaussian_kde\n",
    "xy = features_tsne.T\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "scatter = axes[1].scatter(features_tsne[:, 0], features_tsne[:, 1], \n",
    "                         c=z, s=50, cmap='viridis', alpha=0.6, \n",
    "                         edgecolors='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('t-SNE Dimension 1', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('t-SNE Dimension 2', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('t-SNE Visualization\\n(Density Heatmap)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=axes[1], label='Density')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'tsne_visualization.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nt-SNE visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis\n",
    "\n",
    "Analyze misclassified samples to understand model limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified samples\n",
    "misclassified_mask = (y_test != y_pred)\n",
    "misclassified_indices = np.where(misclassified_mask)[0]\n",
    "correctly_classified_mask = (y_test == y_pred)\n",
    "\n",
    "print(f\"Error Analysis:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total test samples: {len(y_test)}\")\n",
    "print(f\"Correctly classified: {correctly_classified_mask.sum()} ({correctly_classified_mask.sum()/len(y_test)*100:.2f}%)\")\n",
    "print(f\"Misclassified: {len(misclassified_indices)} ({len(misclassified_indices)/len(y_test)*100:.2f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze confidence of predictions\n",
    "pred_confidence = y_pred_probs.max(axis=1)\n",
    "correct_confidence = pred_confidence[correctly_classified_mask]\n",
    "incorrect_confidence = pred_confidence[misclassified_mask]\n",
    "\n",
    "print(f\"\\nPrediction Confidence:\")\n",
    "print(f\"  Correct predictions - Mean: {correct_confidence.mean():.3f}, Std: {correct_confidence.std():.3f}\")\n",
    "print(f\"  Incorrect predictions - Mean: {incorrect_confidence.mean():.3f}, Std: {incorrect_confidence.std():.3f}\")\n",
    "\n",
    "# Visualize confidence distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Confidence histogram\n",
    "axes[0].hist(correct_confidence, bins=50, alpha=0.7, label='Correct', color='green', edgecolor='black')\n",
    "axes[0].hist(incorrect_confidence, bins=50, alpha=0.7, label='Incorrect', color='red', edgecolor='black')\n",
    "axes[0].set_xlabel('Prediction Confidence', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Confidence vs correctness\n",
    "bins = np.linspace(0, 1, 11)\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "correct_by_conf = []\n",
    "total_by_conf = []\n",
    "\n",
    "for i in range(len(bins)-1):\n",
    "    mask = (pred_confidence >= bins[i]) & (pred_confidence < bins[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        correct_by_conf.append((y_test[mask] == y_pred[mask]).sum())\n",
    "        total_by_conf.append(mask.sum())\n",
    "    else:\n",
    "        correct_by_conf.append(0)\n",
    "        total_by_conf.append(1)  # Avoid division by zero\n",
    "\n",
    "accuracy_by_conf = np.array(correct_by_conf) / np.array(total_by_conf)\n",
    "\n",
    "axes[1].plot(bin_centers, accuracy_by_conf, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[1].plot([0, 1], [0, 1], 'r--', alpha=0.5, linewidth=2, label='Perfect Calibration')\n",
    "axes[1].set_xlabel('Prediction Confidence', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Model Calibration Curve', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'error_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze error patterns by subject\n",
    "error_rate_by_subject = []\n",
    "for subject_id in range(n_subjects):\n",
    "    subject_mask = (y_test == subject_id)\n",
    "    if subject_mask.sum() > 0:\n",
    "        error_rate = (y_pred[subject_mask] != subject_id).sum() / subject_mask.sum()\n",
    "        error_rate_by_subject.append(error_rate)\n",
    "    else:\n",
    "        error_rate_by_subject.append(0)\n",
    "\n",
    "# Plot error rates\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(range(n_subjects), error_rate_by_subject, alpha=0.7, edgecolor='black', color='coral')\n",
    "plt.axhline(np.mean(error_rate_by_subject), color='red', linestyle='--', \n",
    "            linewidth=2, label=f'Mean Error Rate: {np.mean(error_rate_by_subject):.3f}')\n",
    "plt.xlabel('Subject ID', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Error Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('Error Rate per Subject', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'error_rate_by_subject.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive discussion report\n",
    "discussion = f\"\"\"\n",
    "{'='*80}\n",
    "PERFORMANCE ANALYSIS AND DISCUSSION\n",
    "{'='*80}\n",
    "\n",
    "1. OVERALL MODEL PERFORMANCE\n",
    "   - Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\n",
    "   - Top-5 Accuracy: {top5_accuracy:.4f} ({top5_accuracy*100:.2f}%)\n",
    "   - F1-Score (Macro): {f1_macro:.4f}\n",
    "   \n",
    "   The model demonstrates {'strong' if accuracy > 0.8 else 'moderate' if accuracy > 0.6 else 'weak'} \n",
    "   performance in identifying individuals from EEG patterns.\n",
    "\n",
    "2. PER-SUBJECT VARIABILITY\n",
    "   - Best performing subject F1-score: {df_metrics['F1_Score'].max():.4f}\n",
    "   - Worst performing subject F1-score: {df_metrics['F1_Score'].min():.4f}\n",
    "   - Standard deviation of F1-scores: {df_metrics['F1_Score'].std():.4f}\n",
    "   \n",
    "   {'High' if df_metrics['F1_Score'].std() > 0.15 else 'Moderate' if df_metrics['F1_Score'].std() > 0.08 else 'Low'} \n",
    "   variability across subjects suggests that some individuals have more distinctive \n",
    "   EEG patterns than others.\n",
    "\n",
    "3. CONFUSION PATTERNS\n",
    "   - Total misclassifications: {len(misclassified_indices)}\n",
    "   - Most confused pair: Subject {confusion_pairs[0][0]} → Subject {confusion_pairs[0][1]} \n",
    "     ({confusion_pairs[0][2]} times)\n",
    "   \n",
    "   The confusion matrix reveals specific subject pairs with similar EEG signatures.\n",
    "\n",
    "4. MODEL CONFIDENCE AND CALIBRATION\n",
    "   - Mean confidence (correct): {correct_confidence.mean():.3f}\n",
    "   - Mean confidence (incorrect): {incorrect_confidence.mean():.3f}\n",
    "   \n",
    "   {'Good' if correct_confidence.mean() - incorrect_confidence.mean() > 0.15 else 'Moderate'} \n",
    "   separation between correct and incorrect prediction confidences indicates \n",
    "   {'well' if correct_confidence.mean() - incorrect_confidence.mean() > 0.15 else 'moderately'}-calibrated predictions.\n",
    "\n",
    "5. KEY INSIGHTS\n",
    "   a) The CNN+RNN hybrid architecture effectively captures both spatial-spectral \n",
    "      features (via CNN) and temporal dependencies (via LSTM) in EEG signals.\n",
    "   \n",
    "   b) Top-5 accuracy of {top5_accuracy*100:.2f}% suggests the model narrows down \n",
    "      candidates effectively, useful for identification in constrained scenarios.\n",
    "   \n",
    "   c) t-SNE visualization shows {'clear' if accuracy > 0.8 else 'some'} clustering \n",
    "      of subjects in the learned feature space, validating that the model learns \n",
    "      discriminative representations.\n",
    "\n",
    "6. POTENTIAL IMPROVEMENTS\n",
    "   - Data augmentation (time-warping, frequency masking)\n",
    "   - Attention mechanisms to focus on discriminative time/frequency regions\n",
    "   - Multi-session training to improve cross-session generalization\n",
    "   - Subject-specific fine-tuning for difficult cases\n",
    "   - Ensemble methods combining multiple model architectures\n",
    "\n",
    "7. PRACTICAL APPLICATIONS\n",
    "   - Biometric authentication using EEG signals\n",
    "   - Security systems requiring brainwave-based identification\n",
    "   - Research on individual differences in brain activity patterns\n",
    "   - Quality control in neuroscience studies (subject identification)\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(discussion)\n",
    "\n",
    "# Save discussion to file\n",
    "with open(os.path.join(FIGURES_DIR, 'performance_discussion.txt'), 'w') as f:\n",
    "    f.write(discussion)\n",
    "\n",
    "print(f\"\\nDiscussion saved to: {os.path.join(FIGURES_DIR, 'performance_discussion.txt')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary report\n",
    "summary_report = {\n",
    "    'model_info': {\n",
    "        'architecture': 'CNN + RNN (Bidirectional LSTM)',\n",
    "        'model_name': latest_model,\n",
    "        'n_subjects': n_subjects,\n",
    "        'total_parameters': model.count_params()\n",
    "    },\n",
    "    'performance_metrics': {\n",
    "        'test_accuracy': float(accuracy),\n",
    "        'top5_accuracy': float(top5_accuracy),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'f1_micro': float(f1_micro),\n",
    "        'f1_weighted': float(f1_weighted)\n",
    "    },\n",
    "    'per_subject_stats': {\n",
    "        'mean_precision': float(precision.mean()),\n",
    "        'mean_recall': float(recall.mean()),\n",
    "        'mean_f1': float(f1.mean()),\n",
    "        'std_f1': float(f1.std()),\n",
    "        'min_f1': float(f1.min()),\n",
    "        'max_f1': float(f1.max())\n",
    "    },\n",
    "    'error_analysis': {\n",
    "        'total_errors': int(len(misclassified_indices)),\n",
    "        'error_rate': float(len(misclassified_indices) / len(y_test)),\n",
    "        'mean_confidence_correct': float(correct_confidence.mean()),\n",
    "        'mean_confidence_incorrect': float(incorrect_confidence.mean())\n",
    "    },\n",
    "    'best_subjects': best_subjects[['Subject_ID', 'F1_Score']].to_dict('records')[:5],\n",
    "    'worst_subjects': worst_subjects[['Subject_ID', 'F1_Score']].to_dict('records')[:5]\n",
    "}\n",
    "\n",
    "# Save summary report\n",
    "summary_file = os.path.join(FIGURES_DIR, 'final_summary_report.json')\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(summary_report, indent=2))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFull report saved to: {summary_file}\")\n",
    "print(f\"All visualizations saved to: {FIGURES_DIR}\")\n",
    "print(\"\\nPerformance analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Performance Analysis Complete!\n",
    "\n",
    "**What we accomplished:**\n",
    "1. Comprehensive evaluation of the CNN+RNN model on test data\n",
    "2. Detailed confusion matrix analysis showing classification patterns\n",
    "3. Per-subject performance breakdown identifying strengths/weaknesses\n",
    "4. t-SNE visualization of learned feature representations\n",
    "5. Error analysis examining misclassifications and model confidence\n",
    "6. Discussion of results and potential improvements\n",
    "\n",
    "**Generated Outputs:**\n",
    "- Multiple visualization plots saved in `figures/` directory\n",
    "- Per-subject metrics CSV file\n",
    "- Performance discussion text file\n",
    "- Comprehensive JSON summary report\n",
    "\n",
    "**Next Steps:**\n",
    "- Review visualizations to understand model behavior\n",
    "- Identify areas for improvement (data augmentation, architecture changes)\n",
    "- Consider ensemble methods or transfer learning\n",
    "- Test on cross-session data for generalization analysis\n",
    "\n",
    "This completes the EEG person identification project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
